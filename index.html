<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">

<title>Image Processing Final</title>
</head>
<h1>
	quiccPrints: Image Processing Final
</h1>

<p>
	Terrance Nance, Elijah Peake, Anthony Turcios
<p>

<div style="text-align: center;">
<img src="finger.jpeg" alt="Finger" style="width:700px;height:500px;">
<img src="print.jpg" alt="Print" style="width:350px;height:500px;">
</div>

<h2>
	Introduction:
</h2>

<p>
&nbsp;&nbsp;&nbsp;&nbsp; quiccPrints gives smartphone users the ability to a photo of a thumb and convert
it into a fingerprint image.  We were initially motivated to pursue this project
because of its useful applications to law enforcement.  Many fingerprinting
devices are large and clunky, some even require an outlet.  In-field police
officers, however, often face life-or-death situations where it is out of the
question to be held back by such a device, reguardless of its usefulness.  If
officers could quickly and easily scan fingerprints and check them in a database,
they would be able to check for people's past criminal history without the need
for non-biometric forms of identification.  For simplicity, we designed quiccPrints
to only focus on capturing thumbprints, but we believe that our software could
be easily expanded upon to include other fingers.
</p>
<p>
&nbsp;&nbsp;&nbsp;&nbsp; Although there have been new, more portable fingerprinting devices created to
satisfy this need, we thought it would be cool to give officers the same ability,
to fingerprint and check for criminals in a database, with only their cellphone.
We were especially drawn to creating this functionality with just a smartphone
because, in the modern day, officers are likely already carrying a smartphone.
Thus, adding this extra functionality to the smartphone gives them one less thing
to carry even if there are already more portable fingerprinting devices.
</p>
<p>
&nbsp;&nbsp;&nbsp;&nbsp; One thing that was consistantly in the back of our minds throughout the project
was that these fingerprints not only had to be extracted from photos, but extracted
well enough from these photos so that they sould later be checked in a database.
It is important to note that such an assessment of fingerprint quality is often
done by organizations like NIST (the National Institute of Standards and
Technology), so, due to time constraints and recources, we were unable to
assess the prints' quality.
</p>
<p>
&nbsp;&nbsp;&nbsp;&nbsp; Due to the large volume of work involved in the project,
each member of our team was assigned a certain group of subtasks that would
countribute to our overall goal of extracting fingerprints.  Because we were unable
to create a data base for our prints and to check for existing prints in the database
due to time constraints, we decided to, instead, output several statistics of interest
for each of our output fingerprints.  Thus, the divided subtasks were as follows:
create a smartphone application to take the photos to be processed; devise an
algorithm that can take in photos and output fingerprint images; devise an
algorithm to determine several key statistics of each fingerprint.
</p>
<p>
&nbsp;&nbsp;&nbsp;&nbsp;
</p>

<h2>
  Methods:
</h2>

<h3>
  Terrance:
</h3>
<p>
&nbsp;&nbsp;&nbsp;&nbsp; My task was to implement feature extraction. In order words, finding a way to describe the contents of the thumbprints that Elijah gave to me. Therefore, I read through different scholarly articles that discussed the best ways to implement feature extraction. I primarily consulted this source for my research: <a href="https://pdfs.semanticscholar.org/49ad/2473ecc1dcbfdfc9a981f5191a1224107ef1.pdf"> https://pdfs.semanticscholar.org/49ad/2473ecc1dcbfdfc9a981f5191a1224107ef1.pdf </a>
</p>
<p>
&nbsp;&nbsp;&nbsp;&nbsp;  As I mentioned earlier, to complete this task, I first decided to conduct some research of my own to find the best approach to take for feature extraction. I read through different scholarly articles that discussed the best ways to implement feature extraction. Once I found a couple of different approaches, I began testing and implementing the different approaches using jupyter notebook. I quickly realized that the Crossing Number Concept was the best algorithm to use. One reason I liked this algorithm is because the function was pretty straightforward to implement. It uses a 3x3 sliding window to scan the local neighborhood (Figure 3). It performs this operation on the outside pixels and returns the value (Figure 4). I can use this chart (Figure 5) to determine what type of point (Figure 6) the center pixel represents. For my actual feature extraction function, I decided to explicitly have variables to represent each position in the 3x3 sliding window because it made the function a lot easier to understand. It also made the operation performed on the outside neighbors clearer and easier to understand because originally, I ran into a lot of problems trying to condense the size of the overall feature extraction method. I successfully implemented this Crossing Number Concept algorithm so that we can now use this algorithm to output a new grayscale image, where the brighter pixels have higher CN values and darker pixels have lower CN values. As a result, we now know the locations of the different types of points. The algorithm I implemented also outputs the total counts of each type of point that are in the given image. I also added a portion to this algorithm that outputs a color image instead of grayscale image where each different type of point represents a different color, so we can better see the results from the algorithm. Since we modified the app so that we only have the top portion/actual thumbprint (Figure 2), I decided that we did not need to add a post-processing element to the algorithm. I stand firm with this decision because now we are only performing operations on the actual thumbprint, instead of the other entire thumb with the extra noisy features (Figure 1). So now our results do not get interfered and we consistently get the same final total counts of each type of point. The only libraries for this process that are used are skimage and matplotlib.

</p>

<h3>
  Elijah:
</h3>
<p>
&nbsp;&nbsp;&nbsp;&nbsp; When I originally started trying to extract a thumbprint for an image, I realized
that I was faced with not only the task of extracting the thumbprint, but also cropping
the image to the thumbprint.  The latter proved to be far more challenging.  Before I could do anything,
I needed to compile a database of people's fingerprints.  To do so, I went to several of my
friends and asked for their consent to take photos of their thumbs, which would be anonymized, to
improve a fingerprint detection algorithm.  I made sure to not only collect fingerprints of students
with a single shade of skin.  I collected thumb photos from frineds of all races, so that performance
would not be biased to a certain race.  All photos were collected with the flash on at roughly the
same distance because this lead to the best performance.
</p>
<p>
  &nbsp;&nbsp;&nbsp;&nbsp; Once I collected the photos, to extract the fingerprint, all I needed to do was use
  local thresholding, with hyperparameters tuned based its performance on several thumbprint photos.
  Local thresholding works by thresholding an image based on luminosity values withing a certain
  window instead of based on the entire image.  This was helpful for extracting fingerprints because
  the peaks of the fingers tend to be brighter than the valleys.  This way, if we set all the values
  of the bright peaks to zero and all the values of the valley to one, we can get a fingerprint.  The
  only thing that needed to be done after this point was to use a median filter to get rid of any noise
  residuals in the photo (to clean up the image), and to use a mask to crop out everything in the photo
  except for the fingerprint (described below).
</p>
<p>
  To
</p>

<h3>
  Anthony:
</h3>

<p>&nbsp;&nbsp;&nbsp;&nbsp; Given that our project revolves around the use of mobile cameras, I decided that I would be using my personal Android device to develop the user interface and backend for the mobile app. I started the development process by designing the various classes that were necessary to work with the Android's hardware. This was necessary because the android SDK is written in Java (so an object oriented design is necessary). Since the device I was working on was an HTC One that was running  <a href="#something">Ice Cream Sandwich</a>, I had to work with a deprecated camera API. After designing the classes and the way that they would interact with each other I began implementing the live camera preview (as would be standard on any mobile device) and the view holder that would display the camera.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp; I had to consider that there was not a great deal of space on my Android's hardware so I refrained from storing captured images. Given this consideration, I had to introduce external storage via a database. So, I used the built in support for Firebase to hold onto images that would have otherwise been in the camera's memory.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp; The last piece of the applicaiton lied in the method of sending the image. However, this is also where many issues arose in the Android version of the application. It turned out that the devices was attempting to compress the image, even though it was not being stored in memory. The likely cause was that trying to send the image to the database was resulting in compression of the image, but when this happened the quality of the image became horrible. To combat this I did implement custom settings for when to atuofocus but there was not much help there.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp; The previous issues lead to the use of an iOS application. This was mostly because the hardware behind the iPhone camera API is much more powerful. I was not familiar with the iOS development workflow but when researching the practices that iOS developers use when buidling applications, I found that it was nearly identical to the methodologies used in Android development. The biggest hurdle in taking on an iOS version of the application became the need to either learn Objective-C of Swift. I found that Swift was far simpler to understand and was similar to JavaScript so I decided to rebuild the application using Swift and the XCode IDE. At this point the challenge became figuring out the various analogs of the custom camera components in the Android application for the iOS application. Fortunately the documentation and other developer's questions on Stack Overflow helped me figure out how to meet our teams needs.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp; The iOS application makes use of the, very flexible, AVFoundation framework. There are other frameworks that are available for using the camera on an iOS device, but this one gave me the most control while not requiring me to write ridiculous amounts of code for the functionality. I decided that the actual interface should be simple enough for anyone to be able to pick up the device and utilize the application. To make the processing of a caputred image easier I added an overlay to the live preview for the application. The overlay is an outline of a generic thumb that is in bright green. There are two majors parts ot the interface: the first is the live preview and the second is the captured image preview. The live preview consits of a button to actually capture the image. The image preview consists of an buttons that allow for sending the image to a server or returning to the previous view.</p>

<p>The workflow is shown best in the following diagram:</p>

<div style="text-align: center;">
  <img src="https://i.pinimg.com/originals/0e/eb/56/0eeb5635ca8ddd32fa3244da56fd46f7.png" style="width:600px;height:600px;">
</div>

<h2>
  Results:
</h2>

<h3>
  Terrance:
</h3>
<p>
kk
</p>

<h3>
  Elijah:
</h3>
<p>
kk
</p>

<h3>
  Anthony:
</h3>
<p>
kk
</p>

<h2>
  Schedule:
</h2>

<h3>
  Elijah:
</h3>

<h4>
  Original Schedule:
</h4>

<ul>
  <strong> Week 5 -- 10/9 </strong>
  <li> Begin developing the edge detection that will be needed for the project (using various IP libraries). </li>
  <li> Research what technologies will make the edge detection most effective given constraints of input images. </li>
  <li> Progress report to the team. </li>
</ul>

<ul>
  <strong> Week 6 -- 10/16 </strong>
  <li> Optimize edge detection algorithm and conversion to binary image. </li>
  <li> Give group update on progress. </li>
  <li> Progress report to the team. </li>
</ul>

<ul>
  <strong> Week 7 -- 10/23 </strong>
  <li> Begin integrating algorithm with application being built. </li>
  <li> Help Terrance with feature extraction and classification algorithm building. </li>
  <li> Progress report to the team. </li>
</ul>

<ul>
  <strong> Week 8 -- 10/30 </strong>
  <li> Tie any loose ends. </li>
  <li> Evaluate where the project is at this point and decide (as a team) viable options for extending the project in time for the final presentation. </li>
  <li> 3MT presentation and report. </li>
</ul>

<h4>
  Actual Schedule:
</h4>

<p>
&nbsp;&nbsp;&nbsp;&nbsp;  Task wise, I followed the original schedule closely; timeline wise, I did not.
I am very fortunate that we gave ourselves a several week pad at the end of the project
timeline to close any loose ends.  I lost a good deal of time trying to translate
all my work into Objective-C++ so that we could integrate my opencv code into the
iphone app, but we never actually ended up integrating all the code because we realize
it cost too much time.  Although it would have been nice to have all the image
processing done locally, having all the code run on an external server proved
easiest task.  It was especially helpful because throughout the process, I ended
up changing my algorithm a lot.  C++, however, is not the best language for prototyping,
so I would prototype in Python, then translate to C++.  Additionally, with C++
I could not use the skimage library, which I ended up using extensively.
</p>
<p>
&nbsp;&nbsp;&nbsp;&nbsp; Asside from this issue, I also ended up spending a lot of time just developing
my algorithm.  I spent countless hours just playing around with all the tools
that we learned in class, like blurring, histogram equalization, edge detection,
masking, etc.  Although I originally planned on finishing the algorithm in the first
two weeks, it ended up taking the entire ~6 weeks to complete.  Part of the
reason it took so long was because as we progressed through the course, I kept
learning new tooks that I wanted to integrate with the algorithm to make it
more robust.
</p>
<p>
&nbsp;&nbsp;&nbsp;&nbsp; A last thing to note is that I never actually helped out
Terrance with his feature extraction task because I became too bogged down with my
own tasks.  Overall, this reflects the our work flow throughout the whole
development process: we made ambitious goals at the start, did not complete
everything we originally wanted too, but kept reassessing goals along the way and
in the end we ended up creating software that I am proud of.
</p>


<h3>
  Terrance:
</h3>

<h4>
  Original Schedule:
</h4>


<ul>
    <strong> Week 6 -- 10/16 </strong>
    <li> Begin researching what makes a thumbprint unique to determine features to be extracted from our thumbprints. </li>
    <li> Testing on a fingerprint taken by Elijah or on one found online, begin implementing feature extraction. </li>
    <li> Progress report to the team. </li>
</ul>

<ul>
    <strong> Week 7 -- 10/23 </strong>
    <li> Continue working on feature extraction and start thinking about what classification algorithm we might want to use, given our set of features. </li>
    <li> Implement a quick and dirty version of the classification algorithm. </li>
    <li> Progress report to the team. </li>
</ul>

<ul>
    <strong> Week 8 -- 10/30 </strong>
    <li> Optimize the algorithm (time permitting). </li>
    <li> Integrate with Anthony’s app. </li>
    <li> 3MT presentation and report. </li>
    <li> Progress report to the team. </li>
</ul>

<h4>
  Actual Schedule:
</h4>

<ul>
    <strong> Week 6 -- 10/16 </strong>
<li> I did not work on the project.
</ul>

<ul>
  <strong> Week 7 -- 10/23 </strong>
<li> I began researching what makes a thumbprint unique to determine features to be extracted from our thumbprints. </li>
<li> I began experimenting the feature extraction methods in scholarly articles with the the thumbprints that Elijah gave me. </li>
</ul>

<ul>
  <strong> Week 8 -- 10/30 </strong>
<li> I successfully implemented the feature extraction algorithm using the Crossing Number Concept. </li>
<li> It outputted the image with new grayscale image, where the brighter pixels have higher CN values and darker pixels have lower CN values. </li>
</ul>

<ul>
  <strong> Week 9 -- 11/7 </strong>
<li> I added a portion to this algorithm that outputs a color image instead of grayscale image where each different type of point represents a different color, so we can better see the results from the algorithm. </li>
<li> I started working on making the algorithm more efficient. </li>
</ul>

<ul>
  <strong> Week 10 – 11/11 </strong>
<li> I came up the faster function that extracted features, printed the statistics, show the different images. </li>
<li> With the help of Anthony and Elijah, we began integrating my code with Anthony’s app. </li>
</ul>

<ul>
<strong> Week 11 – 11/18 </strong>
<li> THANKSGIVING BREAK (so I did not work on the project).  </li>
</ul>

<h3>
  Anthony:
</h3>

<h4>
  Original Schedule:
</h4>

<ul>
  <strong> Week 5 -- 10/9 </strong>
  <li> Begin researching handling camera usage and developing Android mobile application. </li>
  <li> Research Swift development to (possibly) use better camera quality on iOS devices.</li>
  <li> Progress report to the team. </li>
</ul>

<ul>
  <strong> Week 6 -- 10/16 </strong>
  <li> Set up the environment for taking photos (Android). </li>
  <li> Look into camera and internet permissions for Android devices. </li>
  <li> Successfully get the phone to at least take an image and send to computer (any of ours). </li>
  <li> (Possibly) Get the image to be sent to a DB? </li>
  <li> Begin researching how camera can force user to take image of thumb within a thumb outline. </li>
  <li> Like cashing a check via bank app. </li>
  <li> Progress report to the team. </li>
</ul>

<ul>
  <strong> Week 7 -- 10/23 </strong>
  <li> Begin to implement a way of forcing the user to place their thumb into certain region. </li>
  <li> Consider using flash to get this done. </li>
  <li> Possibly go through current code and refactor where applicable. </li>
  <li> Work on formal project progress report (due the following week). </li>
  <li> This includes the 3MT. </li>
</ul>

<ul>
  <strong> Week 8 -- 10/30 </strong>
  <li> 3MT presentation and report. </li>
  <li> Check in with team on next steps for project depending on how much has gotten done. </li>
</ul>

<h4>
  Actual Schedule:
</h4>

<h2>
  Issues:
</h2>

<h3>
  Terrance:
</h3>

<p>
&nbsp;&nbsp;&nbsp;&nbsp;  Originally, when we first divided up the work amongst the three of us. I was originally tasked with coming up with object detection. After a few days and feelings of frustration, we decided that we did not need object detection, since the app was going to take a square image centered around the thumb in the middle. This did not affect my schedule too much since this happened very early on in the semester. Additionally, when I was first trying to implement feature extraction, I ran into a problem when it was trying to thin the image. Initially I did not know how to thin a thumbprint, so I did some research to learn how to actually thin an image. I eventually noticed that under skimage morphology there is a thin operation that outputs the desired skeleton image and I began using that operation to output the thinned thumbprint images. Luckily, it only took me a few days to find this solution. Unfortunately, I later ran into another problem when I was trying make my feature extraction algorithm run faster because at the time I had made separate functions for extracting features, printing the statistics and showing the cool color images. I was able to fix this by having one function that has completes all these tasks more efficiently. This function has parameters so that the user can put either true or false if they want certain options such as printing the statistics or showing the color images that represents the different types of points.
</p>

<h3>
  Elijah:
</h3>

<p>
NONE WE DA BEST
</p>

<h3>
  Anthony:
</h3>

<p>
NONE WE DA BEST
</p>

<h2>
  Future Work:
</h2>

<p>
&nbsp;&nbsp;&nbsp;&nbsp; Although you likely could collect fingerprints from fingers other than thumbs
with the current software, the software has been optimized to work only with
thumbprints.  If we were given more time, we would like to add this additional
functionality.
</p>

<p>
&nbsp;&nbsp;&nbsp;&nbsp; Also, as briefly mentioned earlier, we currently have no evaluation metric of
fingerprint quality, and would likely have to consult an outside agency like
NIST to do so.  This would be another importent future step to take because,
although we can often successfully collect thumbprints with the software, we
currently have no understanding of whether the prints would be considered good
or not.
</p>

<p>
&nbsp;&nbsp;&nbsp;&nbsp; Connected to asessing fingerprint quality, we would ultamately like to give
users the ability to not only take fingerprints from their cellphones, but also
then send the prints to a database to be checked for a match.  This was originally
a key part of our application, but due to the difficulty involved with developing
an iphone app without previous experience and developing a unique fingerprint
extraction algorithm, we faced significant setbacks.
</p>

<h2>
  References:
</h2>
<a href="https://scikit-image.org/"> Scikit-Image </a>
<br>
<br>
<a href="https://opencv.org/"> OpenCV </a>
<br>
<br>
<a href="https://matplotlib.org/"> Matplotlib </a>
<br>
<br>
<a href="https://pdfs.semanticscholar.org/49ad/2473ecc1dcbfdfc9a981f5191a1224107ef1.pdf"> https://pdfs.semanticscholar.org/49ad/2473ecc1dcbfdfc9a981f5191a1224107ef1.pdf </a>
<br>
<br>

</html>
